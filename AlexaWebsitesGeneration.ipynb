{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWebsites(element): #returns True if element is a listed websites\n",
    "    return re.search('<a href=\"/siteinfo/', str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategories(element): #returns True if element is a subcategory\n",
    "    return re.search('<a href=\"/topsites/category/Top/', str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getCategoryPath recursively goes through subcategories and runs getWebsitesList only for the deepest subcategories\n",
    "\n",
    "def getCategoryPath(file_path, current_path, count): \n",
    "    print(current_path)\n",
    "    if count >= 3:\n",
    "        return\n",
    "    \n",
    "    try: #trying to access current url\n",
    "        htmltext = urllib.request.urlopen(\"https://www.alexa.com/topsites/category/Top\" + current_path).read()\n",
    "    except: #catches any error in opening current url\n",
    "        print(\"Error loading page\")\n",
    "        return current_path\n",
    "        \n",
    "    soup = BeautifulSoup(htmltext)\n",
    "\n",
    "    if soup.find(string=re.compile(\"No sites for this category.\")) != None: #do nothing if the smallest sub-category contains no websites\n",
    "        return\n",
    "    texts = soup.findAll(href=True)\n",
    "\n",
    "    categories_html = filter(getCategories, texts) #contains all categories with some syntax noise\n",
    "    categories = []\n",
    "\n",
    "    for text in categories_html: #adds cleaned category string to the categories array\n",
    "        temp = str(text.encode(\"utf-8\"))\n",
    "        if current_path == \"\":\n",
    "            categories.append(re.search('Top/(.+?)\"', temp).group(1))\n",
    "        else:\n",
    "            if(re.search(current_path + '/(.+?)\"', temp) == None): # skips any URLs with weird characters that produces an error with the regular expression\n",
    "                continue\n",
    "            categories.append(re.search(current_path + '/(.+?)\"', temp).group(1))\n",
    "            \n",
    "    if count == 2 or soup.find(string=re.compile(\"Sub-Categories \\(\")) == None: #get the list of top websites of the deepest sub-category\n",
    "        getWebsitesList(file_path, current_path)                        \n",
    "    elif count != 2:\n",
    "        for category in categories: #recurse until it reaches the deepest sub-category\n",
    "            getCategoryPath(file_path, current_path + \"/\" + category , count + 1)    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getWebsitesList inserts top 50 websites of the category_path into csv file\n",
    "\n",
    "def getWebsitesList(file_path, category_path):\n",
    "    htmltext = urllib.request.urlopen(\"https://www.alexa.com/topsites/category/Top\" + category_path).read()\n",
    "\n",
    "    soup = BeautifulSoup(htmltext)\n",
    "    texts = soup.findAll(href=True)\n",
    "\n",
    "    websites = filter(getWebsites, texts)\n",
    "    result = []\n",
    "    if not os.path.exists(file_path):\n",
    "        result.append(['website', 'category'])\n",
    "\n",
    "    for text in websites:\n",
    "        temp = str(text.encode(\"utf-8\"))\n",
    "        result.append([re.search('siteinfo/(.+?)\"', temp).group(1), category_path])\n",
    "    result_np = np.asarray(result)\n",
    "\n",
    "    with open(file_path, 'a') as f:\n",
    "        if not os.path.exists(file_path):\n",
    "            pd.DataFrame(result_np).to_csv(f, header=True)\n",
    "        else:\n",
    "            pd.DataFrame(result_np).to_csv(f, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Computers\n",
      "/Computers/Algorithms\n",
      "/Computers/Artificial_Intelligence\n",
      "/Computers/Artificial_Life\n",
      "/Computers/Bulletin_Board_Systems\n",
      "/Computers/CAD_and_CAM\n",
      "/Computers/Chats_and_Forums\n",
      "/Computers/Companies\n",
      "/Computers/Computer_Science\n",
      "/Computers/Conferences\n",
      "/Computers/Consultants\n",
      "/Computers/Data_Communications\n",
      "/Computers/Data_Formats\n",
      "/Computers/Desktop_Publishing\n",
      "/Computers/Directories\n",
      "/Computers/E-Books\n",
      "/Computers/Education\n",
      "/Computers/Emulators\n",
      "/Computers/Ethics\n",
      "/Computers/FAQs,_Help,_and_Tutorials\n",
      "/Computers/Graphics\n",
      "/Computers/Hacking\n",
      "/Computers/Hardware\n",
      "/Computers/History\n",
      "/Computers/Home_Automation\n",
      "/Computers/Human-Computer_Interaction\n",
      "/Computers/Internet\n",
      "/Computers/Intranet\n",
      "/Computers/Mailing_Lists\n",
      "/Computers/Mobile_Computing\n",
      "/Computers/Multimedia\n",
      "/Computers/News_and_Media\n",
      "/Computers/Open_Source\n",
      "/Computers/Organizations\n",
      "/Computers/Parallel_Computing\n",
      "/Computers/Performance_and_Capacity\n",
      "/Computers/Programming\n",
      "/Computers/Robotics\n",
      "/Computers/Security\n",
      "/Computers/Shopping\n",
      "/Computers/Software\n",
      "/Computers/Speech_Technology\n",
      "/Computers/Supercomputing\n",
      "/Computers/Systems\n",
      "/Computers/Usenet\n",
      "/Computers/Virtual_Reality\n",
      "Exited recursion with:True\n",
      "/Regional\n",
      "/Regional/Africa\n",
      "/Regional/Asia\n",
      "/Regional/Caribbean\n",
      "/Regional/Central_America\n",
      "/Regional/Countries\n",
      "/Regional/Europe\n",
      "/Regional/Middle_East\n",
      "/Regional/North_America\n",
      "/Regional/Oceania\n",
      "/Regional/Polar_Regions\n",
      "/Regional/South_America\n",
      "Exited recursion with:True\n",
      "/Society\n",
      "/Society/Activism\n",
      "/Society/Advice\n",
      "/Society/Crime\n",
      "/Society/Death\n",
      "/Society/Disabled\n",
      "/Society/Ethnicity\n",
      "/Society/Folklore\n",
      "/Society/Future\n",
      "/Society/Gay,_Lesbian,_and_Bisexual\n",
      "/Society/Genealogy\n",
      "/Society/Government\n",
      "/Society/History\n",
      "/Society/Holidays\n",
      "/Society/Issues\n",
      "/Society/Law\n",
      "/Society/Lifestyle_Choices\n",
      "/Society/Military\n",
      "/Society/Organizations\n",
      "/Society/Paranormal\n",
      "/Society/People\n",
      "/Society/Philanthropy\n",
      "/Society/Philosophy\n",
      "/Society/Politics\n",
      "/Society/Relationships\n",
      "/Society/Religion_and_Spirituality\n",
      "/Society/Sexuality\n",
      "/Society/Subcultures\n",
      "/Society/Support_Groups\n",
      "/Society/Transgendered\n",
      "/Society/Work\n",
      "Exited recursion with:True\n"
     ]
    }
   ],
   "source": [
    "##Note: Computers, Regional, and Society categories were too big and produced error 503 from Amazon due to the large number of requests. In order to get their datasets, change the count number from 0 to 1\n",
    "##Note2: Excluded the World category as the sites are not in English\n",
    "\n",
    "main_categories = ['Adult', 'Arts', 'Business', 'Computers', 'Games', 'Health', 'Home', 'Kids and Teens', 'News', 'Recreation', 'Reference', 'Regional', 'Science', 'Shopping', 'Society', 'Sports']\n",
    "#main_categories = ['Computers', 'Regional', 'Society'] #have to rerun these with less subcategory depth since there were too many requests to Amazon\n",
    "\n",
    "for category in main_categories:\n",
    "    filename = 'websites/alexa_websites_' + category + '.csv' #file path that will contain the csv file\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    result = getCategoryPath(filename, \"/\" + category, 1) # Change from count from 0 to 1 for Computers, Regional, and Society categories\n",
    "    print(\"Exited recursion with:\" + str(result))\n",
    "    #time.sleep(2) #avoiding error 503 (where amazon blocks access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
