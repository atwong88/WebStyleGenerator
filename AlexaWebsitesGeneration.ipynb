{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'websites/alexa_websites.csv' #file path that will contain the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWebsites(element): #returns True if element is a listed websites\n",
    "    return re.search('<a href=\"/siteinfo/', str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategories(element): #returns True if element is a subcategory\n",
    "    return re.search('<a href=\"/topsites/category/Top/', str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getCategoryPath recursively goes through subcategories and runs getWebsitesList only for the deepest subcategories\n",
    "\n",
    "def getCategoryPath(current_path): \n",
    "    print(current_path)\n",
    "    \n",
    "    try: #trying to access current url\n",
    "        htmltext = urllib.request.urlopen(\"https://www.alexa.com/topsites/category/Top\" + current_path).read()\n",
    "    except: #catches any error in opening current url\n",
    "        print(\"Error loading page\")\n",
    "        return\n",
    "        \n",
    "    soup = BeautifulSoup(htmltext)\n",
    "    \n",
    "    if soup.find(string=re.compile(\"No sites for this category.\")) != None: #do nothing if the smallest sub-category contains no websites\n",
    "        return\n",
    "    \n",
    "    texts = soup.findAll(href=True)\n",
    "\n",
    "    categories_html = filter(getCategories, texts) #contains all categories with some syntax noise\n",
    "    categories = []\n",
    "\n",
    "    for text in categories_html: #adds cleaned category string to the categories array\n",
    "        temp = str(text.encode(\"utf-8\"))\n",
    "        if current_path == \"\":\n",
    "            categories.append(re.search('Top/(.+?)\"', temp).group(1))\n",
    "        else:\n",
    "            categories.append(re.search(current_path + '/(.+?)\"', temp).group(1))\n",
    "                              \n",
    "    if current_path != \"\" and soup.find(string=re.compile(\"Sub-Categories \\(\")) == None: #get the list of top websites of the deepest sub-category\n",
    "        getWebsitesList(current_path)\n",
    "                              \n",
    "    for category in categories: #recurse until it reaches the deepest sub-category\n",
    "        if category == 'Adult':\n",
    "            continue\n",
    "        getCategoryPath(current_path + \"/\" + category)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getWebsitesList inserts top 50 websites of the category_path into csv file\n",
    "\n",
    "def getWebsitesList(category_path):\n",
    "    try:\n",
    "        htmltext = urllib.request.urlopen(\"https://www.alexa.com/topsites/category/Top\" + category_path).read()\n",
    "    except:\n",
    "        print(\"Error loading page\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(htmltext)\n",
    "    texts = soup.findAll(href=True)\n",
    "\n",
    "    websites = filter(getWebsites, texts)\n",
    "    result = []\n",
    "\n",
    "    for text in websites:\n",
    "        temp = str(text.encode(\"utf-8\"))\n",
    "        result.append([re.search('siteinfo/(.+?)\"', temp).group(1), category_path])\n",
    "    result_np = np.asarray(result)\n",
    "\n",
    "    with open(filename, 'a') as f:\n",
    "        if not os.path.exists(filename):\n",
    "            pd.DataFrame(result_np).to_csv(f, header=True)\n",
    "        else:\n",
    "            pd.DataFrame(result_np).to_csv(f, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Arts\n",
      "/Arts/Animation\n",
      "/Arts/Animation/Anime\n",
      "/Arts/Animation/Anime/Characters\n",
      "/Arts/Animation/Anime/Clubs_and_Organizations\n",
      "/Arts/Animation/Anime/Collectibles\n",
      "/Arts/Animation/Anime/Collectibles/Cels\n",
      "/Arts/Animation/Anime/Collectibles/Models_and_Figures\n",
      "/Arts/Animation/Anime/Collectibles/Models_and_Figures/Action_Figures\n",
      "/Arts/Animation/Anime/Collectibles/Models_and_Figures/Action_Figures/Gundam\n",
      "/Arts/Animation/Anime/Collectibles/Models_and_Figures/Action_Figures/Zoids\n",
      "/Arts/Animation/Anime/Collectibles/Models_and_Figures/Models\n",
      "/Arts/Animation/Anime/Collectibles/Models_and_Figures/Models/Gundam\n",
      "/Arts/Animation/Anime/Collectibles/Shitajiki\n",
      "/Arts/Animation/Anime/Creators\n",
      "/Arts/Animation/Anime/Creators/Anno,_Hideaki\n",
      "/Arts/Animation/Anime/Creators/Ikuhara,_Kunihiko\n",
      "/Arts/Animation/Anime/Creators/Miyazaki,_Hayao\n",
      "/Arts/Animation/Anime/Creators/Studios\n",
      "/Arts/Animation/Anime/Creators/Studios/Studio_Ghibli\n",
      "/Arts/Animation/Anime/Creators/Studios/Studio_Ghibli/Titles\n",
      "/Arts/Animation/Anime/Distribution\n",
      "/Arts/Animation/Anime/Distribution/Companies\n",
      "/Arts/Animation/Anime/Fandom\n",
      "/Arts/Animation/Anime/Fandom/Chats_and_Forums\n",
      "/Arts/Animation/Anime/Fandom/Cliques\n",
      "/Arts/Animation/Anime/Fandom/Conventions\n",
      "/Arts/Animation/Anime/Fandom/Conventions/Organizations\n",
      "/Arts/Animation/Anime/Fandom/Conventions/Reports_and_Photo_Albums\n",
      "/Arts/Animation/Anime/Fandom/Conventions/Schedules\n",
      "/Arts/Animation/Anime/Fandom/Cosplay\n",
      "/Arts/Animation/Anime/Fandom/Fan_Dubbed\n",
      "/Arts/Animation/Anime/Fandom/Fan_Subtitled\n",
      "/Arts/Animation/Anime/Fandom/Fan_Subtitled/Distributors_and_Traders\n",
      "/Arts/Animation/Anime/Fandom/Fan_Subtitled/Subtitlers\n",
      "/Arts/Animation/Anime/Fandom/Fan_Subtitled/Web_Rings\n",
      "/Arts/Animation/Anime/Fandom/Fan_Works\n",
      "Error loading page\n",
      "/Arts/Animation/Anime/Fandom/Webpage_Assistance\n",
      "/Arts/Animation/Anime/Fandom/Webpage_Assistance/Adoption_Centers\n",
      "/Arts/Animation/Anime/Fandom/Webpage_Assistance/Awards\n",
      "/Arts/Animation/Anime/Fandom/Webpage_Assistance/Graphics\n",
      "/Arts/Animation/Anime/Fandom/Webpage_Assistance/Reviews\n",
      "/Arts/Animation/Anime/Games\n",
      "/Arts/Animation/Anime/Games/Roleplaying\n",
      "/Arts/Animation/Anime/Games/Roleplaying/Message_Boards\n",
      "/Arts/Animation/Anime/Games/Roleplaying/Web_Chat\n",
      "/Arts/Animation/Anime/Games/Trading_Cards\n",
      "/Arts/Animation/Anime/Games/Video_Games\n",
      "/Arts/Animation/Anime/Genres\n",
      "/Arts/Animation/Anime/Genres/Children\\'s\n",
      "/Arts/Animation/Anime/Genres/Comedy\n",
      "/Arts/Animation/Anime/Genres/Drama\n",
      "Error loading page\n",
      "/Arts/Animation/Anime/Genres/Fantasy\n",
      "Error loading page\n",
      "/Arts/Animation/Anime/Genres/Magical_Girl\n",
      "/Arts/Animation/Anime/Genres/Martial_Arts\n",
      "/Arts/Animation/Anime/Genres/Mecha\n",
      "/Arts/Animation/Anime/Genres/Mecha/Web_Rings\n",
      "/Arts/Animation/Anime/Genres/Occult\n",
      "/Arts/Animation/Anime/Genres/Romance\n",
      "/Arts/Animation/Anime/Genres/Science_Fiction\n",
      "/Arts/Animation/Anime/Genres/Sentai\n",
      "/Arts/Animation/Anime/Genres/Sports\n",
      "/Arts/Animation/Anime/Genres/Steampunk\n",
      "/Arts/Animation/Anime/Genres/Superhero\n",
      "/Arts/Animation/Anime/Image_Galleries\n",
      "/Arts/Animation/Anime/Image_Galleries/Scans\n",
      "/Arts/Animation/Anime/Image_Galleries/Wallpaper\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "getCategoryPath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
